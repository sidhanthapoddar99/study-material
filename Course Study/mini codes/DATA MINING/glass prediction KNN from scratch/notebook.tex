
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{glass type prediction}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    Attribute Information: 1. Id number: 1 to 214 2. RI: refractive index 3.
Na: Sodium (unit measurement: weight percent in corresponding oxide, as
are attributes 4-10) 4. Mg: Magnesium 5. Al: Aluminum 6. Si: Silicon 7.
K: Potassium 8. Ca: Calcium 9. Ba: Barium 10. Fe: Iron 11. Type of
glass: (class attribute)

\begin{verbatim}
  - 1 building_windows_float_processed
  - 2 building_windows_non_float_processed
  - 3 vehicle_windows_float_processed
  - 4 vehicle_windows_non_float_processed (none in this database)
  - 5 containers
  - 6 tableware
  - 7 headlamps
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{k}{def} \PY{n+nf}{calc}\PY{p}{(}\PY{n}{arr}\PY{p}{,}\PY{n}{test}\PY{p}{)}\PY{p}{:}
             \PY{n}{arr1}\PY{o}{=}\PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{arr}\PY{p}{:}
                 \PY{n}{calc}\PY{o}{=}\PY{l+m+mi}{0}
                 \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
                     \PY{n}{calc}\PY{o}{+}\PY{o}{=}\PY{n+nb}{pow}\PY{p}{(}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n}{test}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
                 \PY{n}{arr1}\PY{o}{+}\PY{o}{=}\PY{p}{[}\PY{p}{[}\PY{n}{calc}\PY{p}{,}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{]}\PY{p}{]}
             \PY{k}{return} \PY{n}{arr1}    
                     
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{k}{def} \PY{n+nf}{KNNcalc}\PY{p}{(}\PY{n}{arr}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{p}{:}
              \PY{n}{arr1}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{9999} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{p}{]}
              \PY{n}{arr2}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{9999} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{p}{]}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{arr}\PY{p}{:}
                  \PY{k}{if} \PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{n+nb}{max}\PY{p}{(}\PY{n}{arr1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                      \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{p}{:}
                          \PY{k}{if}\PY{p}{(}\PY{n+nb}{max}\PY{p}{(}\PY{n}{arr1}\PY{p}{)}\PY{o}{==}\PY{n}{arr1}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                              \PY{n}{arr1}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{=}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                              \PY{n}{arr2}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{=}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                              \PY{k}{break}
              \PY{k}{return} \PY{n}{arr1}\PY{p}{,}\PY{n}{arr2}                
                          
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{k}{def} \PY{n+nf}{accuraccy}\PY{p}{(}\PY{n}{arr}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{p}{:}
              \PY{n}{count}\PY{o}{=}\PY{l+m+mi}{0}
              \PY{k}{for} \PY{n}{i}  \PY{o+ow}{in}  \PY{n}{arr}\PY{p}{:}
                  \PY{k}{if} \PY{n}{i}\PY{o}{==}\PY{n}{n}\PY{p}{:}
                      \PY{n}{count}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
              \PY{k}{return} \PY{n}{count}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{arr}\PY{p}{)}    
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{k}{def} \PY{n+nf}{split}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
              \PY{n}{train}\PY{o}{=}\PY{p}{[}\PY{p}{]}
              \PY{n}{test}\PY{o}{=}\PY{p}{[}\PY{p}{]}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                  \PY{k}{if}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZpc{}}\PY{k}{15}==0):
                      \PY{n}{test}\PY{o}{+}\PY{o}{=}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
                  \PY{k}{else}\PY{p}{:}
                      \PY{n}{train}\PY{o}{+}\PY{o}{=}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
              \PY{k}{return} \PY{n}{train}\PY{p}{,}\PY{n}{test}        
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{k}{def} \PY{n+nf}{pred}\PY{p}{(}\PY{n}{a}\PY{p}{)}\PY{p}{:}
              \PY{n}{arr}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{]}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{a}\PY{p}{:}
                  \PY{n}{arr}\PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
              \PY{n}{flag}\PY{o}{=}\PY{n+nb}{max}\PY{p}{(}\PY{n}{arr}\PY{p}{)}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{:}
                  \PY{k}{if}\PY{p}{(}\PY{n}{arr}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{==}\PY{n}{flag}\PY{p}{)}\PY{p}{:}
                      \PY{k}{return}\PY{p}{(}\PY{n}{i}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}104}]:} \PY{k}{def} \PY{n+nf}{acc}\PY{p}{(}\PY{n}{test}\PY{p}{,}\PY{n}{train}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{p}{:}
              \PY{n}{arr}\PY{o}{=}\PY{p}{[}\PY{p}{]}
              \PY{n}{error}\PY{o}{=}\PY{l+m+mi}{0}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{test}\PY{p}{:}
                  \PY{n+nb}{print}\PY{p}{(}\PY{n}{i}\PY{p}{)}
                  \PY{n}{z}\PY{o}{=}\PY{n}{calc}\PY{p}{(}\PY{n}{train}\PY{p}{,}\PY{n}{i}\PY{p}{)}
                  \PY{n}{arr1}\PY{p}{,}\PY{n}{arr2}\PY{o}{=}\PY{n}{KNNcalc}\PY{p}{(}\PY{n}{z}\PY{p}{,}\PY{n}{n}\PY{p}{)}
                  \PY{n}{acc}\PY{o}{=}\PY{n}{accuraccy}\PY{p}{(}\PY{n}{arr2}\PY{p}{,}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
                  \PY{n}{arr}\PY{o}{+}\PY{o}{=}\PY{p}{[}\PY{n}{acc}\PY{p}{]}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{actual value: }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{  predicted values: }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{arr2}\PY{p}{)}\PY{p}{)}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuraccy :}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{acc}\PY{p}{)}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{k}{if}\PY{p}{(}\PY{n}{pred}\PY{p}{(}\PY{n}{arr2}\PY{p}{)}\PY{o}{!=}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                      \PY{n}{error}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total accuraccy}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{arr}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{)}
              
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total error}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{error}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{)}
              \PY{k}{return}\PY{p}{(}\PY{p}{[}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{arr}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{15}\PY{p}{,}\PY{n}{error}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{l+m+mi}{15}\PY{p}{]}\PY{p}{)}
                  
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{n}{arr}\PY{o}{=}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}
          \PY{n}{arr1}\PY{p}{,}\PY{n}{arr2}\PY{o}{=}\PY{n}{KNNcalc}\PY{p}{(}\PY{n}{arr}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
          \PY{n}{arr2}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}105}]:} [2, 5, 5, 3]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{n}{dataset} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C:}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{Users}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{Sid}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{Desktop}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{python files}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{glass prediction KNN from scratch}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{glass.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{data}\PY{o}{=}\PY{n}{dataset}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
          \PY{n}{train}\PY{p}{,}\PY{n}{test}\PY{o}{=}\PY{n}{split}\PY{p}{(}\PY{n}{data}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}107}]:} \PY{n}{dataset}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}107}]:}      series        A      B     C     D      E     F     G     H     I  RESULT
          0         1  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.00  0.00       1
          1         2  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.00  0.00       1
          2         3  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.00  0.00       1
          3         4  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.00  0.00       1
          4         5  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.00  0.00       1
          5         6  1.51596  12.79  3.61  1.62  72.97  0.64  8.07  0.00  0.26       1
          6         7  1.51743  13.30  3.60  1.14  73.09  0.58  8.17  0.00  0.00       1
          7         8  1.51756  13.15  3.61  1.05  73.24  0.57  8.24  0.00  0.00       1
          8         9  1.51918  14.04  3.58  1.37  72.08  0.56  8.30  0.00  0.00       1
          9        10  1.51755  13.00  3.60  1.36  72.99  0.57  8.40  0.00  0.11       1
          10       11  1.51571  12.72  3.46  1.56  73.20  0.67  8.09  0.00  0.24       1
          11       12  1.51763  12.80  3.66  1.27  73.01  0.60  8.56  0.00  0.00       1
          12       13  1.51589  12.88  3.43  1.40  73.28  0.69  8.05  0.00  0.24       1
          13       14  1.51748  12.86  3.56  1.27  73.21  0.54  8.38  0.00  0.17       1
          14       15  1.51763  12.61  3.59  1.31  73.29  0.58  8.50  0.00  0.00       1
          15       16  1.51761  12.81  3.54  1.23  73.24  0.58  8.39  0.00  0.00       1
          16       17  1.51784  12.68  3.67  1.16  73.11  0.61  8.70  0.00  0.00       1
          17       18  1.52196  14.36  3.85  0.89  71.36  0.15  9.15  0.00  0.00       1
          18       19  1.51911  13.90  3.73  1.18  72.12  0.06  8.89  0.00  0.00       1
          19       20  1.51735  13.02  3.54  1.69  72.73  0.54  8.44  0.00  0.07       1
          20       21  1.51750  12.82  3.55  1.49  72.75  0.54  8.52  0.00  0.19       1
          21       22  1.51966  14.77  3.75  0.29  72.02  0.03  9.00  0.00  0.00       1
          22       23  1.51736  12.78  3.62  1.29  72.79  0.59  8.70  0.00  0.00       1
          23       24  1.51751  12.81  3.57  1.35  73.02  0.62  8.59  0.00  0.00       1
          24       25  1.51720  13.38  3.50  1.15  72.85  0.50  8.43  0.00  0.00       1
          25       26  1.51764  12.98  3.54  1.21  73.00  0.65  8.53  0.00  0.00       1
          26       27  1.51793  13.21  3.48  1.41  72.64  0.59  8.43  0.00  0.00       1
          27       28  1.51721  12.87  3.48  1.33  73.04  0.56  8.43  0.00  0.00       1
          28       29  1.51768  12.56  3.52  1.43  73.15  0.57  8.54  0.00  0.00       1
          29       30  1.51784  13.08  3.49  1.28  72.86  0.60  8.49  0.00  0.00       1
          ..      {\ldots}      {\ldots}    {\ldots}   {\ldots}   {\ldots}    {\ldots}   {\ldots}   {\ldots}   {\ldots}   {\ldots}     {\ldots}
          184     185  1.51115  17.38  0.00  0.34  75.41  0.00  6.65  0.00  0.00       6
          185     186  1.51131  13.69  3.20  1.81  72.81  1.76  5.43  1.19  0.00       7
          186     187  1.51838  14.32  3.26  2.22  71.25  1.46  5.79  1.63  0.00       7
          187     188  1.52315  13.44  3.34  1.23  72.38  0.60  8.83  0.00  0.00       7
          188     189  1.52247  14.86  2.20  2.06  70.26  0.76  9.76  0.00  0.00       7
          189     190  1.52365  15.79  1.83  1.31  70.43  0.31  8.61  1.68  0.00       7
          190     191  1.51613  13.88  1.78  1.79  73.10  0.00  8.67  0.76  0.00       7
          191     192  1.51602  14.85  0.00  2.38  73.28  0.00  8.76  0.64  0.09       7
          192     193  1.51623  14.20  0.00  2.79  73.46  0.04  9.04  0.40  0.09       7
          193     194  1.51719  14.75  0.00  2.00  73.02  0.00  8.53  1.59  0.08       7
          194     195  1.51683  14.56  0.00  1.98  73.29  0.00  8.52  1.57  0.07       7
          195     196  1.51545  14.14  0.00  2.68  73.39  0.08  9.07  0.61  0.05       7
          196     197  1.51556  13.87  0.00  2.54  73.23  0.14  9.41  0.81  0.01       7
          197     198  1.51727  14.70  0.00  2.34  73.28  0.00  8.95  0.66  0.00       7
          198     199  1.51531  14.38  0.00  2.66  73.10  0.04  9.08  0.64  0.00       7
          199     200  1.51609  15.01  0.00  2.51  73.05  0.05  8.83  0.53  0.00       7
          200     201  1.51508  15.15  0.00  2.25  73.50  0.00  8.34  0.63  0.00       7
          201     202  1.51653  11.95  0.00  1.19  75.18  2.70  8.93  0.00  0.00       7
          202     203  1.51514  14.85  0.00  2.42  73.72  0.00  8.39  0.56  0.00       7
          203     204  1.51658  14.80  0.00  1.99  73.11  0.00  8.28  1.71  0.00       7
          204     205  1.51617  14.95  0.00  2.27  73.30  0.00  8.71  0.67  0.00       7
          205     206  1.51732  14.95  0.00  1.80  72.99  0.00  8.61  1.55  0.00       7
          206     207  1.51645  14.94  0.00  1.87  73.11  0.00  8.67  1.38  0.00       7
          207     208  1.51831  14.39  0.00  1.82  72.86  1.41  6.47  2.88  0.00       7
          208     209  1.51640  14.37  0.00  2.74  72.85  0.00  9.45  0.54  0.00       7
          209     210  1.51623  14.14  0.00  2.88  72.61  0.08  9.18  1.06  0.00       7
          210     211  1.51685  14.92  0.00  1.99  73.06  0.00  8.40  1.59  0.00       7
          211     212  1.52065  14.36  0.00  2.02  73.42  0.00  8.44  1.64  0.00       7
          212     213  1.51651  14.38  0.00  1.94  73.61  0.00  8.48  1.57  0.00       7
          213     214  1.51711  14.23  0.00  2.08  73.36  0.00  8.62  1.67  0.00       7
          
          [214 rows x 11 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}108}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
199
15

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{n}{z}\PY{o}{=}\PY{n}{calc}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{p}{[}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.52101}\PY{p}{,} \PY{l+m+mf}{13.64}\PY{p}{,} \PY{l+m+mf}{4.49}\PY{p}{,} \PY{l+m+mf}{1.1}\PY{p}{,} \PY{l+m+mf}{71.78}\PY{p}{,} \PY{l+m+mf}{0.06}\PY{p}{,} \PY{l+m+mf}{8.75}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
          \PY{n}{r1}\PY{p}{,}\PY{n}{r2}\PY{o}{=}\PY{n}{KNNcalc}\PY{p}{(}\PY{n}{z}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{)}
          \PY{n}{accuraccy}\PY{p}{(}\PY{n}{r2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}109}]:} 57.142857142857146
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}110}]:} \PY{n}{bestKNN}\PY{o}{=}\PY{p}{[}\PY{p}{]}
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{:}
              \PY{n}{bestKNN}\PY{o}{+}\PY{o}{=}\PY{p}{[}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{+}\PY{n}{acc}\PY{p}{(}\PY{n}{test}\PY{p}{,}\PY{n}{train}\PY{p}{,}\PY{n}{i}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0]
accuraccy :50.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 1.0]
accuraccy :50.0


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0]
accuraccy :0.0


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0]
accuraccy :100.0


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [5.0, 2.0]
accuraccy :50.0


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0]
accuraccy :100.0


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0]
accuraccy :100.0


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 1.0]
accuraccy :0.0


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0]
accuraccy :100.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 7.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0]
accuraccy :100.0


total accuraccy70.0
total error20.0
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [2.0, 2.0, 1.0]
accuraccy :33.333333333333336


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 3.0, 3.0]
accuraccy :33.333333333333336


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0, 3.0]
accuraccy :0.0


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [3.0, 2.0, 2.0]
accuraccy :66.66666666666667


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 5.0]
accuraccy :66.66666666666667


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0]
accuraccy :100.0


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0]
accuraccy :100.0


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 1.0, 2.0]
accuraccy :0.0


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 5.0]
accuraccy :100.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 7.0, 7.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy66.66666666666667
total error33.333333333333336
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0, 2.0, 1.0]
accuraccy :50.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0, 3.0, 1.0]
accuraccy :25.0


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 3.0, 3.0, 3.0]
accuraccy :25.0


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 3.0, 1.0]
accuraccy :50.0


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 5.0, 5.0]
accuraccy :50.0


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0]
accuraccy :75.0


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 2.0, 2.0, 1.0]
accuraccy :0.0


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 5.0, 5.0]
accuraccy :100.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [2.0, 7.0, 7.0, 7.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy65.0
total error26.666666666666668
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0, 1.0, 1.0, 2.0]
accuraccy :60.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 1.0, 1.0, 3.0, 3.0]
accuraccy :40.0


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0, 1.0, 1.0, 3.0]
accuraccy :40.0


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [3.0, 1.0, 1.0, 2.0, 2.0]
accuraccy :40.0


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 5.0, 5.0]
accuraccy :60.0


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0]
accuraccy :80.0


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0]
accuraccy :80.0


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 2.0, 2.0, 1.0, 1.0]
accuraccy :0.0


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 5.0, 5.0, 5.0]
accuraccy :100.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 7.0, 2.0, 2.0, 7.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy66.66666666666667
total error33.333333333333336
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [2.0, 1.0, 2.0, 2.0, 1.0, 1.0]
accuraccy :50.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 7.0, 3.0, 3.0, 3.0]
accuraccy :33.333333333333336


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 1.0, 3.0, 3.0, 3.0, 1.0]
accuraccy :33.333333333333336


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 1.0, 3.0, 2.0]
accuraccy :50.0


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 5.0, 2.0, 5.0, 2.0, 2.0]
accuraccy :66.66666666666667


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 3.0, 2.0]
accuraccy :66.66666666666667


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0, 2.0]
accuraccy :83.33333333333333


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 1.0, 2.0, 2.0, 3.0, 1.0]
accuraccy :16.666666666666668


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [2.0, 5.0, 5.0, 5.0, 5.0, 5.0]
accuraccy :83.33333333333333


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [2.0, 7.0, 7.0, 7.0, 2.0, 2.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy65.55555555555556
total error26.666666666666668
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0]
accuraccy :57.142857142857146


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0, 2.0, 1.0, 1.0, 3.0, 7.0]
accuraccy :28.571428571428573


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 2.0, 3.0, 1.0, 3.0, 3.0, 1.0]
accuraccy :28.571428571428573


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 1.0, 3.0, 1.0]
accuraccy :57.142857142857146


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 5.0, 5.0, 5.0, 2.0, 2.0, 2.0]
accuraccy :57.142857142857146


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 3.0, 1.0, 2.0, 2.0, 3.0, 2.0]
accuraccy :57.142857142857146


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :85.71428571428571


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 1.0, 3.0, 2.0, 2.0, 1.0, 2.0]
accuraccy :14.285714285714286


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 2.0, 5.0, 5.0, 2.0, 5.0]
accuraccy :71.42857142857143


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 7.0, 7.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy63.80952380952381
total error26.666666666666668
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0]
accuraccy :62.5


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 7.0]
accuraccy :37.5


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0, 3.0, 3.0, 1.0, 3.0, 3.0, 1.0]
accuraccy :37.5


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 3.0, 2.0]
accuraccy :62.5


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 5.0, 5.0, 2.0, 5.0, 2.0, 5.0, 2.0]
accuraccy :50.0


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [3.0, 2.0, 1.0, 2.0, 3.0, 1.0, 2.0, 2.0]
accuraccy :50.0


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :75.0


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 2.0, 3.0, 1.0, 2.0, 1.0, 2.0, 2.0]
accuraccy :12.5


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 2.0, 2.0, 5.0, 5.0, 5.0, 5.0]
accuraccy :75.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 1.0, 2.0, 7.0, 2.0, 2.0, 2.0, 7.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy64.16666666666667
total error20.0
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 2.0, 3.0, 1.0, 2.0, 1.0, 2.0, 1.0]
accuraccy :55.55555555555556


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 3.0, 7.0, 1.0, 2.0, 3.0, 3.0]
accuraccy :44.44444444444444


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [2.0, 1.0, 3.0, 3.0, 1.0, 3.0, 1.0, 7.0, 3.0]
accuraccy :33.333333333333336


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0]
accuraccy :55.55555555555556


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [5.0, 5.0, 2.0, 5.0, 2.0, 5.0, 5.0, 2.0, 2.0]
accuraccy :44.44444444444444


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0]
accuraccy :55.55555555555556


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0]
accuraccy :77.77777777777777


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 1.0, 2.0, 2.0]
accuraccy :11.11111111111111


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [2.0, 5.0, 2.0, 5.0, 5.0, 5.0, 5.0, 5.0, 2.0]
accuraccy :66.66666666666667


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 1.0, 7.0, 2.0, 2.0, 2.0, 2.0, 7.0, 2.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy62.96296296296295
total error26.666666666666668
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0, 1.0, 3.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0]
accuraccy :50.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 7.0, 2.0, 3.0, 1.0, 3.0, 1.0, 2.0, 1.0, 1.0]
accuraccy :40.0


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 3.0, 1.0, 7.0]
accuraccy :40.0


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 7.0, 2.0, 3.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0]
accuraccy :50.0


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [5.0, 5.0, 2.0, 5.0, 5.0, 5.0, 2.0, 2.0, 5.0, 2.0]
accuraccy :40.0


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [3.0, 3.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :60.0


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :80.0


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 1.0, 3.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0]
accuraccy :10.0


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 5.0, 1.0, 5.0, 5.0, 5.0, 2.0, 2.0, 2.0]
accuraccy :60.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 1.0, 2.0, 7.0, 2.0, 2.0, 2.0, 6.0, 2.0, 7.0]
accuraccy :10.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy62.666666666666664
total error20.0
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 3.0, 1.0, 2.0]
accuraccy :54.54545454545455


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0]
accuraccy :90.9090909090909


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0, 7.0, 1.0, 2.0, 3.0, 1.0, 2.0, 1.0, 1.0, 1.0]
accuraccy :45.45454545454545


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 3.0, 1.0, 3.0, 3.0, 7.0, 3.0, 3.0, 2.0, 1.0]
accuraccy :36.36363636363637


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0]
accuraccy :90.9090909090909


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 7.0, 1.0, 2.0, 3.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :54.54545454545455


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 5.0, 5.0, 2.0, 5.0, 2.0, 5.0, 2.0, 5.0, 5.0, 2.0]
accuraccy :45.45454545454545


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 3.0]
accuraccy :63.63636363636363


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 3.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :81.81818181818181


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 1.0, 2.0, 3.0, 2.0]
accuraccy :18.181818181818183


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [2.0, 1.0, 5.0, 5.0, 2.0, 5.0, 2.0, 5.0, 2.0, 5.0, 5.0]
accuraccy :54.54545454545455


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 1.0, 2.0, 7.0, 2.0, 6.0, 2.0, 7.0, 2.0, 2.0, 7.0]
accuraccy :9.090909090909092


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy63.03030303030302
total error26.666666666666668
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [2.0, 1.0, 3.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0]
accuraccy :50.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :91.66666666666667


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 1.0, 3.0, 3.0, 1.0, 7.0, 1.0, 3.0, 2.0, 1.0, 2.0, 1.0]
accuraccy :41.666666666666664


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 3.0, 3.0, 7.0, 3.0, 3.0, 1.0, 3.0, 2.0, 3.0, 1.0]
accuraccy :33.333333333333336


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0]
accuraccy :91.66666666666667


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [3.0, 2.0, 2.0, 7.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0]
accuraccy :58.333333333333336


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [5.0, 2.0, 5.0, 2.0, 2.0, 5.0, 5.0, 2.0, 2.0, 5.0, 2.0, 5.0]
accuraccy :50.0


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 1.0, 3.0, 2.0, 2.0, 2.0]
accuraccy :66.66666666666667


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]
accuraccy :83.33333333333333


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0]
accuraccy :16.666666666666668


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 2.0, 1.0, 2.0, 5.0, 2.0, 5.0, 2.0, 2.0, 5.0, 5.0]
accuraccy :50.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 1.0, 2.0, 6.0, 2.0, 7.0, 2.0, 2.0, 7.0, 2.0, 7.0, 7.0]
accuraccy :8.333333333333334


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy62.777777777777786
total error20.0
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 2.0]
accuraccy :53.84615384615385


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :92.3076923076923


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 7.0, 3.0, 3.0, 3.0, 2.0, 2.0, 1.0]
accuraccy :38.46153846153846


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0, 3.0, 1.0, 1.0, 3.0, 1.0, 7.0, 3.0, 3.0, 3.0, 3.0, 3.0]
accuraccy :30.76923076923077


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0]
accuraccy :84.61538461538461


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 3.0, 1.0, 2.0, 1.0, 2.0, 7.0, 3.0, 1.0, 2.0, 2.0, 2.0]
accuraccy :53.84615384615385


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [5.0, 2.0, 5.0, 2.0, 2.0, 2.0, 5.0, 5.0, 5.0, 2.0, 5.0, 6.0, 2.0]
accuraccy :46.15384615384615


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0]
accuraccy :61.53846153846154


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0]
accuraccy :76.92307692307692


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 3.0, 2.0, 1.0]
accuraccy :15.384615384615385


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [2.0, 2.0, 5.0, 5.0, 5.0, 5.0, 1.0, 2.0, 2.0, 5.0, 5.0, 5.0, 2.0]
accuraccy :53.84615384615385


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [7.0, 1.0, 2.0, 7.0, 7.0, 6.0, 2.0, 7.0, 2.0, 7.0, 2.0, 7.0, 2.0]
accuraccy :7.6923076923076925


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy61.02564102564102
total error20.0
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0]
accuraccy :50.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :92.85714285714286


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 2.0, 2.0, 1.0, 1.0, 3.0, 1.0, 3.0, 1.0, 7.0, 3.0, 1.0, 2.0, 1.0]
accuraccy :42.857142857142854


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0, 2.0, 3.0, 3.0, 7.0, 1.0, 1.0, 3.0, 3.0, 3.0, 1.0, 1.0, 1.0]
accuraccy :35.714285714285715


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0]
accuraccy :78.57142857142857


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 7.0, 3.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0]
accuraccy :50.0


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [5.0, 6.0, 2.0, 5.0, 5.0, 2.0, 2.0, 5.0, 5.0, 2.0, 2.0, 2.0, 5.0, 5.0]
accuraccy :42.857142857142854


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0]
accuraccy :64.28571428571429


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 2.0]
accuraccy :71.42857142857143


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0]
accuraccy :14.285714285714286


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [1.0, 2.0, 2.0, 2.0, 5.0, 5.0, 5.0, 2.0, 2.0, 5.0, 5.0, 5.0, 5.0, 5.0]
accuraccy :57.142857142857146


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [1.0, 1.0, 2.0, 7.0, 7.0, 7.0, 2.0, 2.0, 6.0, 7.0, 2.0, 7.0, 7.0, 2.0]
accuraccy :7.142857142857143


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy60.476190476190474
total error26.666666666666668

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}111}]:} \PY{n}{acc}\PY{p}{(}\PY{n}{test}\PY{p}{,}\PY{n}{train}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[1.0, 1.52101, 13.64, 4.49, 1.1, 71.78, 0.06, 8.75, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 2.0, 2.0, 1.0]
accuraccy :50.0


[16.0, 1.5176100000000001, 12.81, 3.54, 1.23, 73.24, 0.58, 8.39, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[31.0, 1.51768, 12.65, 3.56, 1.3, 73.08, 0.61, 8.69, 0.0, 0.14, 1.0]
actual value: 1.0  predicted values: [1.0, 1.0, 1.0, 1.0]
accuraccy :100.0


[46.0, 1.5190000000000001, 13.49, 3.48, 1.35, 71.95, 0.55, 9.0, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [3.0, 3.0, 3.0, 1.0]
accuraccy :25.0


[61.0, 1.51905, 13.6, 3.62, 1.11, 72.64, 0.14, 8.76, 0.0, 0.0, 1.0]
actual value: 1.0  predicted values: [1.0, 3.0, 3.0, 3.0]
accuraccy :25.0


[76.0, 1.5159, 13.02, 3.58, 1.51, 73.12, 0.69, 7.96, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[91.0, 1.51841, 12.93, 3.74, 1.11, 72.28, 0.64, 8.96, 0.0, 0.22, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 3.0, 1.0]
accuraccy :50.0


[106.0, 1.52475, 11.45, 0.0, 1.88, 72.19, 0.81, 13.24, 0.0, 0.34, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 5.0, 5.0]
accuraccy :50.0


[121.0, 1.51844, 13.25, 3.76, 1.32, 72.4, 0.58, 8.42, 0.0, 0.0, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 1.0, 2.0]
accuraccy :75.0


[136.0, 1.51789, 13.19, 3.9, 1.3, 72.33, 0.55, 8.44, 0.0, 0.28, 2.0]
actual value: 2.0  predicted values: [2.0, 2.0, 2.0, 2.0]
accuraccy :100.0


[151.0, 1.51665, 13.14, 3.45, 1.76, 72.48, 0.6, 8.38, 0.0, 0.17, 3.0]
actual value: 3.0  predicted values: [2.0, 2.0, 2.0, 1.0]
accuraccy :0.0


[166.0, 1.5217100000000001, 11.56, 1.88, 1.56, 72.86, 0.47, 11.41, 0.0, 0.0, 5.0]
actual value: 5.0  predicted values: [5.0, 5.0, 5.0, 5.0]
accuraccy :100.0


[181.0, 1.51299, 14.4, 1.74, 1.54, 74.55, 0.0, 7.59, 0.0, 0.0, 6.0]
actual value: 6.0  predicted values: [2.0, 7.0, 7.0, 7.0]
accuraccy :0.0


[196.0, 1.51545, 14.14, 0.0, 2.68, 73.39, 0.08, 9.07, 0.61, 0.05, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


[211.0, 1.51685, 14.92, 0.0, 1.99, 73.06, 0.0, 8.4, 1.59, 0.0, 7.0]
actual value: 7.0  predicted values: [7.0, 7.0, 7.0, 7.0]
accuraccy :100.0


total accuraccy65.0
total error26.666666666666668

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}111}]:} [65.0, 26.666666666666668]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}114}]:} \PY{c+c1}{\PYZsh{}the best result is found using 8 neighbours having 64\PYZpc{} acc match and 20\PYZpc{} error}
          \PY{n}{bestKNN}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}114}]:} [[2, 70.0, 20.0],
           [3, 66.66666666666667, 33.333333333333336],
           [4, 65.0, 26.666666666666668],
           [5, 66.66666666666667, 33.333333333333336],
           [6, 65.55555555555556, 26.666666666666668],
           [7, 63.80952380952381, 26.666666666666668],
           [8, 64.16666666666667, 20.0],
           [9, 62.96296296296295, 26.666666666666668],
           [10, 62.666666666666664, 20.0],
           [11, 63.03030303030302, 26.666666666666668],
           [12, 62.777777777777786, 20.0],
           [13, 61.02564102564102, 20.0],
           [14, 60.476190476190474, 26.666666666666668]]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}119}]:} \PY{n}{z}\PY{o}{=}\PY{n}{calc}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{p}{[}\PY{l+m+mf}{176.0}\PY{p}{,} \PY{l+m+mf}{1.52119}\PY{p}{,} \PY{l+m+mf}{12.97}\PY{p}{,} \PY{l+m+mf}{0.33}\PY{p}{,} \PY{l+m+mf}{1.51}\PY{p}{,} \PY{l+m+mf}{73.39}\PY{p}{,} \PY{l+m+mf}{0.13}\PY{p}{,} \PY{l+m+mf}{11.27}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.28}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{)}
          \PY{n}{arr1}\PY{p}{,}\PY{n}{arr2}\PY{o}{=}\PY{n}{KNNcalc}\PY{p}{(}\PY{n}{z}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}120}]:} \PY{n}{arr2}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}120}]:} [5.0, 2.0, 6.0, 5.0, 5.0, 2.0, 5.0, 5.0]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}121}]:} \PY{n}{pred}\PY{p}{(}\PY{n}{arr2}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}121}]:} 5
\end{Verbatim}
            

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
