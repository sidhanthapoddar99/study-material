{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB MINING LAB ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a program that returns the unary, elias_delta, elias_gamma and Golomb coding of a given decimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:03:12.742948Z",
     "start_time": "2020-02-28T16:03:12.729982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimal: Unary       : E.Gamma    : E.Delta    : Goulomb   \n",
      "0      : 0           : 0          : 0          : 000       \n",
      "1      : 10          : 100        : 1000       : 001       \n",
      "2      : 110         : 1100       : 11000      : 010       \n",
      "3      : 1110        : 1101       : 11001      : 1000      \n",
      "4      : 11110       : 111000     : 110100     : 1001      \n",
      "5      : 111110      : 111001     : 110101     : 1010      \n",
      "6      : 1111110     : 111010     : 110110     : 11000     \n",
      "7      : 11111110    : 111011     : 110111     : 11001     \n",
      "8      : 111111110   : 11110000   : 111000000  : 11010     \n",
      "9      : 1111111110  : 11110001   : 111000001  : 111000    \n",
      "10     : 11111111110 : 11110010   : 111000010  : 111001    \n"
     ]
    }
   ],
   "source": [
    "from math import log,ceil\n",
    "log2 = lambda x: log(x,2)\n",
    "\n",
    "def binary(x,l=1):\n",
    "    fmt = '{0:0%db}' % l\n",
    "    return fmt.format(x)\n",
    "\n",
    "def unary(x):\n",
    "    return x*'1'+'0'\n",
    "\n",
    "def elias_generic(lencoding, x):\n",
    "    if x == 0: \n",
    "        return '0'\n",
    "    l = 1+int(log2(x))\n",
    "    a = x - 2**(int(log2(x)))\n",
    "    k = int(log2(x))\n",
    "    return lencoding(l) + binary(a,k)\n",
    "\n",
    "def golomb(b, x):\n",
    "    q = int((x) / b)\n",
    "    r = int((x) % b)\n",
    "    l = int(ceil(log2(b)))\n",
    "    #print q,r,l\n",
    "    return unary(q) + binary(r, l)\n",
    "\n",
    "def elias_gamma(x):\n",
    "    return elias_generic(unary, x)\n",
    "\n",
    "def elias_delta(x):\n",
    "    return elias_generic(elias_gamma,x)\n",
    "\n",
    "print(\"%-7s: %-11s : %-10s : %-10s : %-10s\" %(\"Decimal\",\"Unary\",\"E.Gamma\",\"E.Delta\",\"Goulomb\"))\n",
    "for i in range(11):\n",
    "    print (\"%-7d: %-11s : %-10s : %-10s : %-10s\" %(i, unary(i),elias_gamma(i),elias_delta(i), golomb(3,i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a program to implement document indexing which can return a set of resultant documents for any arbitrary search word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:05:57.697098Z",
     "start_time": "2020-02-28T16:05:57.691080Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def bold(txt):\n",
    "    return ('\\x1b[1m%s\\x1b[0m' % txt)\n",
    "\n",
    "DATA = [\n",
    "{\n",
    "'title': 'Modern Technology',\n",
    "'description': \"\"\"Science is always useful for us. We cannot think of our modern\n",
    "lifestyle without the blessing of science and modern technology. Now-a-days technology\n",
    "is being used in the sphere of education, industry, medical treatment, information\n",
    "and communication technology , household activity and space technology. Modern society\n",
    "is a blessing of technology. The apply of technology had made our life easy and\n",
    "comfortable. A huge number of daily necessary product are made by the help of modern\n",
    "technology. Such as medicine, cosmetic products, gmo food , automobile, cloths etc.\n",
    "Computer, radio, television, refrigerator, airplane, internet, cell phone, telephone\n",
    "etc are the gift of modern technology and science. \"\"\"\n",
    "},\n",
    "{\n",
    "'title': 'Science',\n",
    "'description': \"\"\"Science has really done wonders for all of us and our lives.\n",
    "Science has discovered laws, has invented many new products. Engineers, doctors, and\n",
    "scientists have made a number of gadgets and appliances for use at homes, in offices\n",
    "or in industries. Science continues to make our life easier, healthier and safer.\n",
    "Science is leading to side effects like global warming, pollution, chemical weapons\n",
    "etc. Unfortunately, science cannot prevent these side effects.\"\"\"\n",
    "},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:06:06.489473Z",
     "start_time": "2020-02-28T16:06:06.477504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Modern Technology',\n",
       "  'description': 'Science is always useful for us. We cannot think of our modern\\nlifestyle without the blessing of science and modern technology. Now-a-days technology\\nis being used in the sphere of education, industry, medical treatment, information\\nand communication technology , household activity and space technology. Modern society\\nis a blessing of technology. The apply of technology had made our life easy and\\ncomfortable. A huge number of daily necessary product are made by the help of modern\\ntechnology. Such as medicine, cosmetic products, gmo food , automobile, cloths etc.\\nComputer, radio, television, refrigerator, airplane, internet, cell phone, telephone\\netc are the gift of modern technology and science. '},\n",
       " {'title': 'Science',\n",
       "  'description': 'Science has really done wonders for all of us and our lives.\\nScience has discovered laws, has invented many new products. Engineers, doctors, and\\nscientists have made a number of gadgets and appliances for use at homes, in offices\\nor in industries. Science continues to make our life easier, healthier and safer.\\nScience is leading to side effects like global warming, pollution, chemical weapons\\netc. Unfortunately, science cannot prevent these side effects.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:08:50.446921Z",
     "start_time": "2020-02-28T16:08:50.438944Z"
    }
   },
   "outputs": [],
   "source": [
    "SPLIT_RE = re.compile(r'[^a-zA-Z0-9]')\n",
    "\n",
    "def tokenize(text):\n",
    "    yield from SPLIT_RE.split(text)\n",
    "\n",
    "def text_only(tokens):\n",
    "    for t in tokens:\n",
    "        if t.isalnum():\n",
    "            yield t\n",
    "\n",
    "def lowercase(tokens):\n",
    "    for t in tokens:\n",
    "        yield t.lower()\n",
    "\n",
    "def stem(tokens):\n",
    "    for t in tokens :\n",
    "        if t.endswith('ly'):\n",
    "            t = t[:-2]\n",
    "        yield t\n",
    "\n",
    "        \n",
    "SYNONYMS = {'rapid': 'quick',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:13:42.433298Z",
     "start_time": "2020-02-28T16:13:42.413351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for \"\u001b[1mscience\u001b[0m\" using \u001b[1mAND\u001b[0m in all fields\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1mScience\u001b[0m found with score of \u001b[1m6\u001b[0m\n",
      "\u001b[1mModern Technology\u001b[0m found with score of \u001b[1m3\u001b[0m\n",
      "\n",
      "\n",
      "Search for \"\u001b[1mtecnology\u001b[0m\" using \u001b[1mAND\u001b[0m in all fields\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Search for \"\u001b[1museful\u001b[0m\" using \u001b[1mAND\u001b[0m in all fields\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1mModern Technology\u001b[0m found with score of \u001b[1m1\u001b[0m\n",
      "\n",
      "\n",
      "Search for \"\u001b[1mof\u001b[0m\" using \u001b[1mAND\u001b[0m in all fields\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1mModern Technology\u001b[0m found with score of \u001b[1m8\u001b[0m\n",
      "\u001b[1mScience\u001b[0m found with score of \u001b[1m2\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def synonyms(tokens):\n",
    "    for t in tokens:\n",
    "        yield SYNONYMS.get(t, t)\n",
    "\n",
    "def analyze(text):\n",
    "    tokens = tokenize(text)\n",
    "    for token_filter in (text_only, lowercase, stem, synonyms):\n",
    "        tokens = token_filter(tokens)\n",
    "    yield from tokens\n",
    "\n",
    "def index_docs(docs, *fields):\n",
    "    index = defaultdict(lambda: defaultdict(Counter))\n",
    "    for id, doc in enumerate(docs):\n",
    "        for field in fields:\n",
    "            for token in analyze(doc[field]):\n",
    "                index[field][token][id] += 1\n",
    "    return index\n",
    "\n",
    "def combine_and(*args):\n",
    "    if not args:\n",
    "        return Counter()\n",
    "    out = args[0].copy()\n",
    "    for c in args[1:]:\n",
    "        for doc_id in list(out):\n",
    "            if doc_id not in c:\n",
    "                del out[doc_id]\n",
    "            else:\n",
    "                out[doc_id] += c[doc_id]\n",
    "    return out\n",
    "\n",
    "def combine_or(*args):\n",
    "    if not args:\n",
    "        return Counter()\n",
    "    out = args[0].copy()\n",
    "    for c in args[1:]:\n",
    "        out.update(c)\n",
    "    return out\n",
    "\n",
    "COMBINE = {'OR': combine_or,'AND': combine_and,}\n",
    "\n",
    "def search_in_fields(index, query, fields):\n",
    "    for t in analyze(query):\n",
    "        yield COMBINE['OR'](*(index[f][t] for f in fields))\n",
    "\n",
    "def search(index, query, operator='AND', fields=None):\n",
    "    combine = COMBINE[operator]\n",
    "    return combine(*(search_in_fields(index, query, fields or index.keys())))\n",
    "\n",
    "def query(index, query, operator='AND', fields=None):\n",
    "    print('Search for \"%s\" using %s in %s' % (bold(query), bold(operator), fields or'all fields'))\n",
    "    print('-'*80)\n",
    "    ids = search(index, query, operator, fields)\n",
    "    for doc_id, score in ids.most_common():\n",
    "        print('%s found with score of %s' % (bold(DATA[doc_id]['title']), bold(score)))\n",
    "    print('\\n')\n",
    "\n",
    "index = index_docs(DATA, 'title', 'description')\n",
    "query(index, 'science')\n",
    "query(index, 'tecnology')\n",
    "query(index, 'useful')\n",
    "query(index, 'of')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Consider the following two sentences\n",
    "## This is a simple sentence, but we need to find tfidf.\n",
    "## This is similar to previous sentence, no need to find tfidf.\n",
    "### Find TF MATRIX, IDF values of each term and finally TF*IDF MATRIX.\n",
    "### Find cosine similarity also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:14:38.026569Z",
     "start_time": "2020-02-28T16:14:30.398793Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:20:47.924507Z",
     "start_time": "2020-02-28T16:20:47.861675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********TF**************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This</th>\n",
       "      <th>a</th>\n",
       "      <th>but</th>\n",
       "      <th>find</th>\n",
       "      <th>is</th>\n",
       "      <th>need</th>\n",
       "      <th>no</th>\n",
       "      <th>previous</th>\n",
       "      <th>sentence</th>\n",
       "      <th>similar</th>\n",
       "      <th>simple</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   This  a  but  find  is  need  no  previous  sentence  similar  simple  \\\n",
       "0     1  1    1     1   1     1   0         0         1        0       1   \n",
       "1     1  0    0     1   1     1   1         1         1        1       0   \n",
       "\n",
       "   tfidf  to  we  \n",
       "0      1   1   1  \n",
       "1      1   2   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********Normalied TF of docA***********\n",
      "\n",
      "*********Normalied TF of docB***********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'previous': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'simple': 0.0,\n",
       " 'find': 0.09090909090909091,\n",
       " 'to': 0.18181818181818182,\n",
       " 'tfidf': 0.09090909090909091,\n",
       " 'similar': 0.09090909090909091,\n",
       " 'need': 0.09090909090909091,\n",
       " 'but': 0.0,\n",
       " 'sentence': 0.09090909090909091,\n",
       " 'no': 0.09090909090909091,\n",
       " 'a': 0.0,\n",
       " 'This': 0.09090909090909091,\n",
       " 'we': 0.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************IDF***************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'previous': 0.3010299956639812,\n",
       " 'is': 0.0,\n",
       " 'simple': 0.3010299956639812,\n",
       " 'find': 0.0,\n",
       " 'to': 0.0,\n",
       " 'tfidf': 0.0,\n",
       " 'similar': 0.3010299956639812,\n",
       " 'need': 0.0,\n",
       " 'but': 0.3010299956639812,\n",
       " 'sentence': 0.0,\n",
       " 'no': 0.3010299956639812,\n",
       " 'a': 0.3010299956639812,\n",
       " 'This': 0.0,\n",
       " 'we': 0.3010299956639812}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************IDF***************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This</th>\n",
       "      <th>a</th>\n",
       "      <th>but</th>\n",
       "      <th>find</th>\n",
       "      <th>is</th>\n",
       "      <th>need</th>\n",
       "      <th>no</th>\n",
       "      <th>previous</th>\n",
       "      <th>sentence</th>\n",
       "      <th>similar</th>\n",
       "      <th>simple</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   This         a       but  find   is  need        no  previous  sentence  \\\n",
       "0   0.0  0.027366  0.027366   0.0  0.0   0.0  0.000000  0.000000       0.0   \n",
       "1   0.0  0.000000  0.000000   0.0  0.0   0.0  0.027366  0.027366       0.0   \n",
       "\n",
       "    similar    simple  tfidf   to        we  \n",
       "0  0.000000  0.027366    0.0  0.0  0.027366  \n",
       "1  0.027366  0.000000    0.0  0.0  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Score [*] 0.5803329846765686\n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "import math\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "docA = \"This is a simple sentence, but we need to find tfidf.\"\n",
    "docB = \"This is similar to previous sentence, no need to find tfidf.\"\n",
    "exclude = set(string.punctuation)\n",
    "docA=''.join(ch for ch in docA if ch not in exclude)\n",
    "docB=''.join(ch for ch in docB if ch not in exclude)\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "wordSet = set(bowA).union(set(bowB))\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "for word in bowA:\n",
    "    wordDictA[word]+=1\n",
    "for word in bowB:\n",
    "    wordDictB[word]+=1\n",
    "print(\"\\n***********TF**************\")\n",
    "display(pd.DataFrame([wordDictA, wordDictB]))\n",
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "tfBowB = computeTF(wordDictB, bowB)\n",
    "print(\"\\n*********Normalied TF of docA***********\")\n",
    "print(\"\\n*********Normalied TF of docB***********\")\n",
    "display(tfBowB)\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "print(\"\\n****************IDF***************\")\n",
    "display(idfs)\n",
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "print(\"\\n****************IDF***************\")\n",
    "display(pd.DataFrame([tfidfBowA, tfidfBowB]))\n",
    "train_set = [docA,docB]\n",
    "# Set up the vectoriser, passing in the stop words\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopWords)\n",
    "# Apply the vectoriser to the training set\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)\n",
    "# Print the score\n",
    "print (\"\\nSimilarity Score [*]\",cosine_similarity(tfidf_matrix_train[0:1],\n",
    "tfidf_matrix_train)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement PAGE RANK ALGORITHM. Take input for adjacency matrix (no need to visualise the directed graph), find stochastic matrix, find transpose of it. Consider dumping factor 0.7. Consider initial P values as all 1s. You can consider 5 nodes. Calculate page rank until 2 iterations and display the ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:23:41.410016Z",
     "start_time": "2020-02-28T16:23:41.384062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Graph:\n",
      " [[0 0 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [1 0 0 1 0]\n",
      " [1 0 0 0 1]\n",
      " [0 1 0 0 0]]\n",
      "\n",
      "Initial rank\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "Iteration 1\n",
      "[[1.  ]\n",
      " [1.  ]\n",
      " [1.7 ]\n",
      " [0.65]\n",
      " [0.65]]\n",
      "\n",
      "Iteration 2\n",
      "Final : \n",
      " [[0.2245]\n",
      " [0.151 ]\n",
      " [0.34  ]\n",
      " [0.179 ]\n",
      " [0.1055]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.2245, 0.151 , 0.34  , 0.179 , 0.1055]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "def pagerank(A, p=0.7, max_iter=2,tol=1e-06, personalize=None, reverse=False):\n",
    "    print(\"Initial Graph:\\n\",A)\n",
    "    if reverse:\n",
    "        A = A.T\n",
    "    n, _ = A.shape\n",
    "    r = sp.asarray(A.sum(axis=1)).reshape(-1)\n",
    "    k = r.nonzero()[0]\n",
    "    D_1 = sprs.csr_matrix((1 / r[k], (k, k)), shape=(n, n))\n",
    "    if personalize is None:\n",
    "        personalize = sp.ones(n)\n",
    "        \n",
    "    personalize = personalize.reshape(n, 1)\n",
    "    s = (personalize / personalize.sum()) * n\n",
    "    \n",
    "    z_T = (((1 - p) * (r != 0) + (r == 0)) / n)[sp.newaxis, :]\n",
    "    W = p * A.T @ D_1\n",
    "    x = s\n",
    "    oldx = sp.zeros((n, 1))\n",
    "    iteration = 0\n",
    "    print(\"\\nInitial rank\")\n",
    "    \n",
    "    while sp.linalg.norm(x - oldx) > tol:\n",
    "        oldx = x\n",
    "        print (x)\n",
    "        x = W @ x + s @ (z_T @ x)\n",
    "        iteration += 1\n",
    "        print(\"\\nIteration\",iteration)\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    x = x / sum(x)\n",
    "    print('Final : \\n',x)\n",
    "    return x.reshape(-1)\n",
    "\n",
    "mat=np.matrix([[0, 0, 1, 0, 0],\n",
    "               [0, 0, 1, 0, 0],\n",
    "               [1, 0, 0, 1, 0],\n",
    "               [1, 0, 0, 0, 1],\n",
    "               [0, 1, 0, 0, 0]])\n",
    "pagerank(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. To identify the best hub and authority for the given adjacency matrix. Calculate the hubs and authority score using hits algorithm for k=3. The adjacency matrix is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:31:53.051356Z",
     "start_time": "2020-02-28T16:31:53.032405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=0 \n",
      "Hubs:\n",
      "[[1 1 1 1]]\n",
      "Authorities:\n",
      "[[1 1 2 4]]\n",
      "\n",
      "\n",
      "For k=  1\n",
      "Hubs:\n",
      "[7, 6, 5, 4]\n",
      "Authorities:\n",
      "[1, 1, 2, 4]\n",
      "\n",
      "\n",
      "For k=  2\n",
      "Hubs:\n",
      "[0.624, 0.535, 0.445, 0.356]\n",
      "Authorities:\n",
      "[0.213, 0.213, 0.426, 0.853]\n",
      "\n",
      "\n",
      "For k=  3\n",
      "Hubs:\n",
      "[0.624, 0.535, 0.445, 0.356]\n",
      "Authorities:\n",
      "[0.213, 0.213, 0.426, 0.853]\n",
      "\n",
      "Best Hub is  N1\n",
      "Best Authority is  N4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "def new(a):\n",
    "    su=0\n",
    "    v=[]\n",
    "    for j in a:\n",
    "        su+=j**2\n",
    "    for j in a:\n",
    "        v+=[round(j/sqrt(su),3)]\n",
    "    return v\n",
    "nodes={0:'N1',1:'N2',2:'N3',3:'N4'}\n",
    "\n",
    "mat=np.matrix([[0,1,1,1],\n",
    "               [0,0,1,1],\n",
    "               [1,0,0,1],\n",
    "               [0,0,0,1]])\n",
    "n=4\n",
    "k=3\n",
    "outdegree=[]\n",
    "indegree=[]\n",
    "u=np.matrix([[1],[1],[1],[1]])\n",
    "\n",
    "for i in range(n):\n",
    "    out,ins=0,0\n",
    "    for j in range(n):\n",
    "        if i==j and mat[i,j]==1:\n",
    "            out+=1\n",
    "            ins+=1\n",
    "        elif mat[i,j]==1:\n",
    "            out+=1\n",
    "        else:\n",
    "            ins+=1\n",
    "    outdegree+=[out]\n",
    "    indegree+=[ins]\n",
    "print('For k=0 \\nHubs:')\n",
    "print(u.transpose())\n",
    "print('Authorities:')\n",
    "matt=mat.transpose()\n",
    "v=matt*u\n",
    "print(v.transpose())\n",
    "u=mat*v\n",
    "z=[]\n",
    "y=[]\n",
    "finy,finz=[],[]\n",
    "\n",
    "for j in range(n):\n",
    "    y+=[u[j,0]]\n",
    "for j in range(n):\n",
    "    z+=[v[j,0]]\n",
    "for i in range(k):\n",
    "    print('\\n\\nFor k= ',i+1)\n",
    "    print('Hubs:')\n",
    "    print(y)\n",
    "    print('Authorities:')\n",
    "    print(z)\n",
    "    finy,finz=y,z\n",
    "    y=new(y)\n",
    "    z=new(z)\n",
    "    \n",
    "maxy=max(y)\n",
    "print(\"\\nBest Hub is \",nodes[y.index(maxy)])\n",
    "\n",
    "maxz=max(z)\n",
    "print(\"Best Authority is \",nodes[z.index(maxz)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
